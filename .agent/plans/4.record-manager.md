# Module 4: Record Manager

⚠️ **Medium** -- 5 tasks

**Delivers:** Content-hash-based chunk deduplication. Re-uploading the same file skips unchanged chunks (zero embedding API calls), inserts new ones, deletes stale ones. Re-uploading a file with an existing filename reuses the existing document record. No new dependencies (uses `hashlib` from stdlib).

---

## Context

The current ingestion pipeline (`process_document()` in `backend/app/ingestion/service.py`) blindly inserts **all** chunks every time. Re-uploading the same file creates duplicate chunks, polluting retrieval. There is no mechanism to detect that a filename was already uploaded -- a new document record is always created.

The Record Manager solves this by:
1. Adding a `content_hash` (SHA-256) column to `document_chunks`
2. On re-processing, comparing new chunk hashes against existing ones for the same document
3. Only embedding and inserting truly new chunks; deleting stale ones
4. Detecting duplicate filenames at upload time and reusing the existing document record

---

## Task 1: SQL Migration -- Add `content_hash` Column
✅ Simple

**Create:** `backend/supabase/migrations/003_record_manager.sql`

```sql
-- Add content_hash column for deduplication
alter table public.document_chunks add column content_hash text;

-- Unique index: same document cannot have two chunks with identical content
create unique index idx_document_chunks_doc_hash
  on public.document_chunks(document_id, content_hash);

-- Index for efficient ordered chunk retrieval per document
create index idx_document_chunks_doc_index
  on public.document_chunks(document_id, chunk_index);

-- Backfill existing chunks using pgcrypto (available in Supabase)
update public.document_chunks
  set content_hash = encode(digest(content, 'sha256'), 'hex')
  where content_hash is null;

-- Enforce NOT NULL after backfill
alter table public.document_chunks alter column content_hash set not null;
```

---

## Task 2: Create Record Manager Module
⚠️ Medium

**Create:** `backend/app/ingestion/record_manager.py`

Core functions:
- `compute_content_hash(content: str) -> str` -- SHA-256 hex digest
- `fetch_existing_chunks(document_id: str) -> list[dict]` -- returns `id, chunk_index, content_hash` from DB
- `reconcile_chunks(document_id, user_id, new_chunks, embeddings) -> ReconciliationResult` -- compares hashes, returns `to_insert`, `to_delete`, `skipped` counts
- `apply_reconciliation(result) -> dict` -- executes DB inserts/deletes, returns summary

**Algorithm** (hash-based, not index-based -- handles content shifting when paragraphs are added/removed):
1. Fetch existing chunks for document → build `{content_hash: chunk_record}` lookup
2. For each new chunk: compute hash → if exists in lookup, skip; else mark for insert
3. Any existing hashes not matched → mark for delete (stale)
4. Apply: delete stale, batch-insert new (batches of 50)

**Patterns:** `@traceable` decorators, `ReconciliationResult` dataclass, `logging`

---

## Task 3: Modify Ingestion Service to Use Record Manager
⚠️ Medium

**Modify:** `backend/app/ingestion/service.py`

Replace the blind-insert logic in `process_document()` with:

1. Compute content hashes for all new chunks
2. Fetch existing chunk hashes from DB
3. Identify which chunks are truly new (hash not in existing set)
4. **Only call `generate_embeddings_batch()` for new chunks** (key optimization -- re-upload of identical file = zero embedding API calls)
5. Call `reconcile_chunks()` + `apply_reconciliation()`
6. Compute final chunk count as `existing - deleted + inserted`

**Key imports to add:** `from app.ingestion.record_manager import compute_content_hash, fetch_existing_chunks, reconcile_chunks, apply_reconciliation`

---

## Task 4: Handle Duplicate Filename at Upload
⚠️ Medium

**Modify:** `backend/app/ingestion/router.py`

Before creating a new document record, check if a document with the same filename already exists for this user:

```python
existing = supabase.table("documents").select("id, storage_path")
    .eq("user_id", user.id).eq("filename", file.filename).execute()
```

If found:
- Reuse the existing `doc_id` and `storage_path`
- Delete old file from storage (best-effort), upload new file to same path
- Reset document status to `"pending"`, clear `error_message`
- Trigger `process_document()` background task (record manager handles chunk dedup)

If not found:
- Existing flow unchanged (new UUID, new record)

This is what makes dedup work end-to-end -- without it, each upload creates a new `document_id` so the record manager would never find existing chunks to compare against.

---

## Task 5: Validation + Progress Update
✅ Simple

**Test matrix:**
| Scenario | Expected |
|---|---|
| Upload new file | All chunks inserted, `content_hash` populated |
| Re-upload identical file | 0 inserts, 0 deletes, all skipped, no embedding API calls |
| Re-upload modified file | Some skipped, new inserted, stale deleted |
| Re-upload completely different content | All old deleted, all new inserted |
| Different filename | New document record, independent chunks |

**Modify:** `PROGRESS.md` -- Update Module 4 checklist

---

## Files Changed

| File | Action |
|------|--------|
| `backend/supabase/migrations/003_record_manager.sql` | **Create** -- content_hash column, unique index, backfill |
| `backend/app/ingestion/record_manager.py` | **Create** -- hash, reconcile, apply (~90 lines) |
| `backend/app/ingestion/service.py` | Modify -- replace blind insert with reconciliation flow |
| `backend/app/ingestion/router.py` | Modify -- duplicate filename detection, document reuse |
| `PROGRESS.md` | Modify -- Module 4 checklist |

**No new dependencies.** Uses `hashlib` (stdlib) + existing `supabase`, `langsmith`.
