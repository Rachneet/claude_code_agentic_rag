# Module 5: Metadata Extraction

## Complexity: ⚠️ Medium

## Context

Documents are currently ingested (chunked, embedded, stored) without any structured metadata. The `document_chunks.metadata` jsonb column exists but is always `{}`. Retrieval is pure vector similarity with no filtering capability. Module 5 adds LLM-powered metadata extraction during ingestion and metadata-filtered retrieval.

## Architecture

- **Document-level extraction, chunk-level propagation.** The LLM is called once per document (first ~4000 chars), producing a `DocumentMetadata` Pydantic model. This is stored on `documents.metadata` and propagated to each chunk's `metadata` field.
- **Extraction runs during ingestion** — after chunking, before embedding. Non-streaming `chat_completion` call. Graceful fallback if LLM returns bad JSON.
- **Metadata filtering in the RPC** — `match_document_chunks` gets an optional `metadata_filter jsonb` parameter using the `@>` containment operator.

---

## Tasks

### Task 1: Create metadata extraction module
**Create** `backend/app/ingestion/metadata.py`
- `DocumentMetadata` Pydantic model: `title`, `document_type` (article/report/tutorial/notes/email/code/data/other), `topics` (list[str], 3-5), `entities` (list[str]), `language` (ISO 639-1), `summary` (2-3 sentences)
- `EXTRACTION_SYSTEM_PROMPT` — instructs LLM to return JSON matching the schema
- `extract_document_metadata(text, filename) -> DocumentMetadata` — calls `get_provider().chat_completion()` with first 4000 chars, `temperature=0.1`, `max_tokens=512`. Strips markdown code fences, parses JSON, validates with Pydantic. Falls back to `_fallback_metadata(filename)` on any error.
- `_fallback_metadata(filename)` — returns minimal valid `DocumentMetadata`
- Decorated with `@traceable` for LangSmith

### Task 2: SQL migration
**Create** `backend/supabase/migrations/004_metadata_extraction.sql`
- `ALTER TABLE documents ADD COLUMN metadata jsonb DEFAULT '{}'`
- `CREATE INDEX idx_document_chunks_metadata ... USING gin (metadata)` on `document_chunks`
- `CREATE INDEX idx_documents_metadata ... USING gin (metadata)` on `documents`
- `CREATE OR REPLACE FUNCTION match_document_chunks(...)` — same signature + new `metadata_filter jsonb DEFAULT NULL` parameter. Adds `AND (metadata_filter IS NULL OR dc.metadata @> metadata_filter)` to WHERE clause.

### Task 3: Integrate into ingestion pipeline
**Modify** `backend/app/ingestion/service.py`
- Import `extract_document_metadata` from metadata module
- Add `filename` parameter to `process_document(doc_id, user_id, storage_path, filename)`
- After `chunk_text()`, call `extract_document_metadata(text, filename)`
- Store metadata on documents row: `supabase.table("documents").update({"metadata": metadata_dict}).eq("id", doc_id).execute()`
- Propagate subset (`document_type`, `topics`, `title`, `language`) into each chunk's `metadata` dict before hash computation

**Modify** `backend/app/ingestion/router.py`
- Pass `file.filename` as 4th arg to `process_document` in both new-document and re-upload paths

### Task 4: Update retrieval for metadata filtering
**Modify** `backend/app/ingestion/retrieval.py`
- Add `metadata_filter: dict | None = None` param to `search_documents()`
- Pass `metadata_filter` to the RPC call when provided
- Update `format_retrieval_context()` — include document title and type in source labels

### Task 5: Update chat tools
**Modify** `backend/app/chat/tools.py`
- Add optional `document_type` and `topic` string parameters to `SEARCH_DOCUMENTS_TOOL`
- Update `execute_tool()` to build `metadata_filter` dict from these params and pass to `search_documents()`

**Modify** `backend/app/chat/service.py`
- Update `HF_RAG_SYSTEM_PROMPT_TEMPLATE` to mention document titles/types in context

### Task 6: Frontend updates
**Modify** `frontend/src/types/index.ts`
- Add `DocumentMetadata` interface
- Add `metadata: DocumentMetadata | null` to `Document`

**Modify** `backend/app/models/schemas.py`
- Add `DocumentMetadataResponse` Pydantic model
- Add `metadata: Optional[DocumentMetadataResponse] = None` to `DocumentResponse`

**Modify** `frontend/src/components/documents/DocumentList.tsx`
- Show metadata badges below filename: document_type as outline badge, first 3 topics as secondary badges
- Only show when `status === "completed"` and metadata exists

**Modify** `frontend/src/components/documents/DocumentsView.tsx`
- Add `typeFilter` state, derive unique types from documents
- Filter documents by selected type before passing to `DocumentList`
- Render filter dropdown above document list

### Task 7: Update progress
**Modify** `PROGRESS.md` — mark Module 5 tasks

---

## Files Summary

| File | Action |
|------|--------|
| `backend/app/ingestion/metadata.py` | Create |
| `backend/supabase/migrations/004_metadata_extraction.sql` | Create |
| `backend/app/ingestion/service.py` | Modify |
| `backend/app/ingestion/router.py` | Modify |
| `backend/app/ingestion/retrieval.py` | Modify |
| `backend/app/chat/tools.py` | Modify |
| `backend/app/chat/service.py` | Modify |
| `backend/app/models/schemas.py` | Modify |
| `frontend/src/types/index.ts` | Modify |
| `frontend/src/components/documents/DocumentList.tsx` | Modify |
| `frontend/src/components/documents/DocumentsView.tsx` | Modify |
| `PROGRESS.md` | Modify |

No new dependencies (Python or frontend).

---

## Verification
1. Run migration 004 in Supabase SQL Editor
2. Start backend, upload a text document, verify `documents.metadata` is populated
3. Check `document_chunks.metadata` has `document_type`, `topics`, `title`, `language`
4. Test retrieval with filter: `search_documents("query", user_id, metadata_filter={"document_type": "tutorial"})`
5. Test chat with Gemini/OpenRouter — ask "search my tutorials about X" and verify LLM uses filter params
6. Verify frontend shows metadata badges after document processing completes
7. Verify type filter dropdown works
