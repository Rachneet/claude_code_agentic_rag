# Module 3: Multi-Provider Support (OpenRouter)

⚠️ **Medium** -- 5 tasks

**Delivers:** OpenRouter as a third LLM provider with full tool calling support, capability-based chat service (any provider with tool support uses tool calling, not just Gemini), zero new dependencies (uses `httpx` already in requirements).

---

## Context

Module 2 built the provider abstraction layer (`LLMProvider` ABC, factory, HF + Gemini implementations). The chat service currently hard-codes `settings.llm_provider.lower() == "gemini"` to decide whether to use tool calling for RAG. Module 3 adds OpenRouter (OpenAI-compatible API, 400+ models) and generalizes the chat service to use a `supports_tools` property instead of provider name checks.

**Key design decisions:**
- **httpx over openai SDK** — project rule is "raw SDK calls only", httpx is already a dependency
- **Tool format conversion inside provider** — `tools.py` stays provider-agnostic, OpenRouter wraps in `{"type":"function","function":{...}}` internally
- **`supports_tools` property** — capability-based branching replaces name-based checks in chat service

---

## Task 1: Add OpenRouter Config
✅ Simple

**Modify:**
- `backend/app/config.py` — Add `openrouter_api_key: str = ""`, `openrouter_model: str = "google/gemini-2.5-flash"`, `openrouter_base_url: str = "https://openrouter.ai/api/v1"`
- `backend/.env.example` — Add OpenRouter env vars, update LLM_PROVIDER comment to show three options

**Validation:** App starts without OpenRouter env vars set (defaults to empty string).

---

## Task 2: Implement OpenRouterProvider
⚠️ Medium

**Create:** `backend/app/llm/openrouter.py`

All 4 `LLMProvider` methods using raw `httpx`:

- `chat_completion()` — Sync `httpx.post` to `/chat/completions`, return `choices[0].message.content`
- `chat_completion_stream()` — Sync generator parsing SSE lines + `_next_or_sentinel` async wrapper
- `chat_completion_with_tools()` — Non-streaming, `_convert_tools()` wraps in OpenAI format, parse `tool_calls` response
- `chat_completion_stream_with_tools()` — Detect tools → execute → append results (OpenAI format: assistant `tool_calls` msg + `tool` role msgs) → stream final
- `supports_tools` property → `True`

**Patterns:** `_SENTINEL` + `_next_or_sentinel`, `@traceable`, `from __future__ import annotations`, `run_in_executor`

**Validation:** Provider instantiates, methods match ABC signatures.

---

## Task 3: Register in Factory
✅ Simple

**Modify:** `backend/app/llm/factory.py` — Add `elif provider_name == "openrouter"` branch, update error message to show all three providers.

**Validation:** `get_provider()` returns `OpenRouterProvider` when `LLM_PROVIDER=openrouter`.

---

## Task 4: Generalize Chat Service Tool-Calling Logic
⚠️ Medium

**Modify:**
- `backend/app/llm/base.py` — Add `supports_tools` property (returns `False`)
- `backend/app/llm/gemini.py` — Override `supports_tools` → `True`
- `backend/app/chat/service.py` — Change `settings.llm_provider.lower() == "gemini"` to `provider.supports_tools`

**Validation:** Chat service uses tool calling for any provider with `supports_tools == True`, falls back to context injection for providers without.

---

## Task 5: E2E Validation + Progress Update
✅ Simple

**Test:** OpenRouter chat + RAG tool calling, Gemini no regression, HF no regression
**Modify:** `PROGRESS.md` — Update Module 3 checklist

---

## Files Changed

| File | Action |
|------|--------|
| `backend/app/config.py` | Modify — add 3 OpenRouter settings |
| `backend/.env.example` | Modify — add OpenRouter env vars |
| `backend/app/llm/openrouter.py` | **Create** — full provider (~150 lines) |
| `backend/app/llm/factory.py` | Modify — add openrouter branch |
| `backend/app/llm/base.py` | Modify — add `supports_tools` property |
| `backend/app/llm/gemini.py` | Modify — override `supports_tools` |
| `backend/app/chat/service.py` | Modify — `provider.supports_tools` instead of name check |
| `PROGRESS.md` | Modify — Module 3 checklist |

**No new dependencies.**
