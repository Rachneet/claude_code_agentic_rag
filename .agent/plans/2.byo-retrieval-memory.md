# Module 2: BYO Retrieval + Memory

ğŸ”´ **Complex** -- 11 tasks

**Delivers:** LLM provider abstraction (HF + Gemini switchable via env var), document ingestion with file upload UI, chunking/embedding/pgvector pipeline, vector similarity search, Gemini tool calling for RAG, Supabase Realtime for ingestion status.

**User Decisions:**
- LLM providers: Provider abstraction layer (HF + Gemini, configurable via `LLM_PROVIDER` env var)
- Embeddings: HuggingFace Inference API (`sentence-transformers/all-MiniLM-L6-v2`, 384 dimensions)

---

## Context

Module 1 delivered a working chat app with HuggingFace Inference, threads, SSE streaming, and LangSmith tracing. Module 2 adds the "R" in RAG: document ingestion, vector storage, and retrieval-augmented chat. It also introduces provider abstraction to support Gemini (with tool calling) alongside the existing HF backend.

---

## Task 1: Database Schema Update (Migration 002)
âœ… Simple

Enable pgvector, create `documents` and `document_chunks` tables with RLS, add similarity search function, enable Realtime on `documents`.

**Create:**
- `backend/supabase/migrations/002_documents_and_vectors.sql`

**Key details:**
- `documents`: id, user_id, filename, file_size, mime_type, status (pending/processing/completed/failed), chunk_count, storage_path, error_message, created_at, updated_at
- `document_chunks`: id, document_id, user_id, content, chunk_index, embedding vector(384), token_count, metadata jsonb, created_at
- HNSW index on embedding column (vector_cosine_ops)
- RLS: 4 policies per table (auth.uid() = user_id)
- `match_document_chunks` RPC function for cosine similarity search
- `alter publication supabase_realtime add table public.documents`
- Auto-update trigger on documents.updated_at

**Validation:** Run SQL in Supabase SQL Editor. Verify tables, RLS policies, pgvector extension, HNSW index, and Realtime publication all exist.

---

## Task 2: Supabase Storage Setup
âœ… Simple

Create `documents` storage bucket with RLS policies (users can only access `{user_id}/` folder).

**Manual steps in Supabase Dashboard:**
1. Create private bucket named `documents` (10MB limit, text MIME types)
2. Add storage policies via SQL: upload/read/delete scoped to `(storage.foldername(name))[1] = auth.uid()::text`

**Storage path convention:** `{user_id}/{document_id}/{filename}`

**Validation:** Bucket appears in dashboard, policies visible, test upload scoped to user folder.

---

## Task 3: LLM Provider Abstraction Layer
âš ï¸ Medium

Create abstract provider interface, refactor HF client into it, add factory function.

**Create:**
- `backend/app/llm/base.py` -- `LLMProvider` ABC with methods: `chat_completion()`, `chat_completion_stream()`, `chat_completion_with_tools()`, `chat_completion_stream_with_tools()`
- `backend/app/llm/factory.py` -- `get_provider()` returns configured provider based on `LLM_PROVIDER` env var

**Modify:**
- `backend/app/llm/huggingface.py` -- Refactor module-level functions into `HuggingFaceProvider(LLMProvider)` class
- `backend/app/config.py` -- Add: `llm_provider: str = "huggingface"`, `google_api_key: str = ""`, `gemini_model: str = "gemini-2.5-flash"`, `hf_embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2"`
- `backend/.env.example` -- Add new env vars

**Key design:** Universal `messages` format (list of role/content dicts). Each provider translates internally. Tools follow Gemini-compatible function declaration format. HF provider raises `NotImplementedError` for tool methods.

**Validation:** `get_provider()` returns correct provider based on env var. Existing chat flow still works end-to-end after refactor.

---

## Task 4: Gemini Provider Implementation
âš ï¸ Medium

Implement `GeminiProvider(LLMProvider)` using `google-genai` SDK.

**Create:**
- `backend/app/llm/gemini.py`

**Key details:**
- `_convert_messages()`: Extract system instruction separately (Gemini handles it differently), map `assistant` â†’ `model` role
- Streaming via `generate_content_stream` + `run_in_executor` (sync SDK, like HF)
- Tool calling: non-streaming call to detect tool calls, execute them, then stream final response
- `@traceable` decorators for LangSmith

**Modify:**
- `backend/requirements.txt` -- Add `google-genai`

**Validation:** Set `LLM_PROVIDER=gemini`, test non-streaming, streaming, and tool calling. Verify LangSmith traces.

---

## Task 5: Update Chat Service for Provider Abstraction
âš ï¸ Medium

Refactor `chat/service.py` to use provider factory instead of direct HF imports.

**Modify:**
- `backend/app/chat/service.py` -- Replace `from app.llm.huggingface import ...` with `from app.llm.factory import get_provider`. Use singleton provider. Move `SYSTEM_PROMPT` to service level.

**Validation:** Chat works with both `LLM_PROVIDER=huggingface` and `LLM_PROVIDER=gemini`. SSE streaming, auto-title, LangSmith traces all work.

---

## Task 6: Embedding Service + Chunking Logic
âš ï¸ Medium

Create text chunking and HF embedding functions -- core building blocks for ingestion.

**Create:**
- `backend/app/ingestion/__init__.py`
- `backend/app/ingestion/embeddings.py` -- `generate_embedding(text) -> list[float]`, `generate_embeddings_batch(texts) -> list[list[float]]` using HF `InferenceClient.feature_extraction()`
- `backend/app/ingestion/chunking.py` -- `chunk_text(text, chunk_size=500, chunk_overlap=100) -> list[TextChunk]` with sentence boundary awareness

**Key details:**
- Embedding model: `all-MiniLM-L6-v2` (384 dims, matches DB schema)
- Character-based chunking with sentence boundary detection (split at `. `, `! `, `? `, `\n\n`)
- `TextChunk` Pydantic model: content, chunk_index, token_count, metadata
- `@traceable` decorators for LangSmith

**Validation:** Chunk a 2000-char text â†’ multiple chunks with overlap. Embed "Hello world" â†’ 384-dim float list.

---

## Task 7: Document Ingestion Pipeline (Backend)
âš ï¸ Medium

Upload endpoint, Supabase Storage, background processing pipeline, list/delete endpoints.

**Create:**
- `backend/app/ingestion/router.py` -- `POST /api/documents/upload`, `GET /api/documents`, `GET /api/documents/{id}`, `DELETE /api/documents/{id}`
- `backend/app/ingestion/service.py` -- `process_document()` background task: download file â†’ extract text â†’ chunk â†’ embed â†’ store in pgvector â†’ update status

**Modify:**
- `backend/app/models/schemas.py` -- Add `DocumentResponse`
- `backend/app/main.py` -- Register ingestion router
- `backend/requirements.txt` -- Add `python-multipart`

**Processing flow:**
1. Validate file type/size
2. Upload to Supabase Storage at `{user_id}/{doc_id}/{filename}`
3. Create document record (status=pending)
4. Background task: download â†’ chunk â†’ embed â†’ insert chunks â†’ update status
5. Status updates trigger Supabase Realtime

**Validation:** Upload `.txt` file, verify status progression (pending â†’ processing â†’ completed), verify chunks in DB, delete removes everything.

---

## Task 8: Vector Search / Retrieval Service
âœ… Simple

Semantic similarity search via pgvector.

**Create:**
- `backend/app/ingestion/retrieval.py` -- `search_documents(query, user_id, threshold, count) -> list[dict]`, `format_retrieval_context(chunks) -> str`

**Flow:** Embed query â†’ call `match_document_chunks` RPC â†’ return ranked chunks.

**Validation:** Upload document, search with related query â†’ relevant chunks returned. Unrelated query with high threshold â†’ empty. Different user â†’ empty (data isolation).

---

## Task 9: RAG Chat Integration (Tool Calling)
âš ï¸ Medium

Wire retrieval into chat. Gemini uses tool calling; HF uses context injection.

**Create:**
- `backend/app/chat/tools.py` -- `SEARCH_DOCUMENTS_TOOL` definition, `RAG_TOOLS` list, `execute_tool()` function

**Modify:**
- `backend/app/chat/service.py` -- Updated `stream_chat_response()`:
  - Gemini: pass tools to provider, handle tool call loop (search â†’ inject results â†’ stream answer)
  - HF: pre-retrieve context, inject into system prompt, stream normally
  - Updated system prompts for RAG mode

**Validation:** Upload doc about topic X. Ask about X with Gemini â†’ tool called, context used. Ask with HF â†’ context injected. Ask unrelated â†’ answers from knowledge. Streaming works throughout.

---

## Task 10: Ingestion UI (Frontend)
âš ï¸ Medium

Document management interface with upload, list, real-time status, and view switching.

**Create:**
- `frontend/src/components/documents/DocumentsView.tsx` -- Main view with Realtime subscription
- `frontend/src/components/documents/FileUpload.tsx` -- Drag-and-drop via react-dropzone
- `frontend/src/components/documents/DocumentList.tsx` -- Document table with status badges, delete

**Modify:**
- `frontend/src/App.tsx` -- Add `activeView` state (chat | documents), render appropriate view
- `frontend/src/components/layout/Header.tsx` -- Add view switcher (Chat / Documents buttons)
- `frontend/src/lib/api.ts` -- Add `apiUploadDocument()`, `apiListDocuments()`, `apiDeleteDocument()`
- `frontend/src/types/index.ts` -- Add `Document` interface, `AppView` type

**Install:**
- `npm install react-dropzone`
- `npx shadcn@latest add progress badge alert dialog dropdown-menu`

**Realtime subscription:** Subscribe to `postgres_changes` on `documents` table filtered by user_id. Update local state on UPDATE events (status changes).

**Validation:** View switching works. Upload file â†’ appears with pending status â†’ auto-updates to completed. Delete works. Unsupported file type shows error.

---

## Task 11: End-to-End Validation + Progress Update
âœ… Simple

Full flow testing across both providers, ingestion, retrieval, Realtime, and RLS.

**Modify:**
- `PROGRESS.md` -- Update Module 2 checklist

**Test matrix:**
- Both providers: chat, streaming, auto-title
- Ingestion: upload, process, status updates via Realtime
- RAG: tool calling (Gemini), context injection (HF)
- Data isolation: RLS enforcement
- LangSmith: traces for chat, embeddings, retrieval

---

## Dependency Graph

```
Task 1 (DB Schema) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                     â”‚
    v                                     â”‚
Task 2 (Storage)     Task 3 (Provider)    â”‚
                         â”‚                â”‚
                         v                â”‚
                     Task 4 (Gemini)      â”‚
                         â”‚                â”‚
                         v                â”‚
                     Task 5 (Chat refactor)
                                          â”‚
Task 6 (Embeddings+Chunking) â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    v
Task 7 (Ingestion Pipeline) â—„â”€â”€ Task 5
    â”‚
    v
Task 8 (Vector Search)
    â”‚
    v
Task 9 (RAG Integration) â—„â”€â”€â”€â”€ Task 5
    â”‚
    v
Task 10 (Ingestion UI) â—„â”€â”€â”€â”€â”€â”€ Task 7
    â”‚
    v
Task 11 (E2E Validation)
```

**Parallel tracks:** Tasks 1-2 (DB/Storage) can run in parallel with Tasks 3-4-5 (Provider abstraction).

---

## New Packages

**pip:** `google-genai`, `python-multipart`
**npm:** `react-dropzone`
**shadcn:** `progress`, `badge`, `alert`, `dialog`, `dropdown-menu`
